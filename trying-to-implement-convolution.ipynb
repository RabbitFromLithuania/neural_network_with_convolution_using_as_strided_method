{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34877,"sourceType":"datasetVersion","datasetId":27352}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom numpy.lib.stride_tricks import as_strided\nimport torch\nimport torch.nn.functional as F\nimport copy\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-30T13:44:07.124140Z","iopub.execute_input":"2025-04-30T13:44:07.124361Z","iopub.status.idle":"2025-04-30T13:44:13.124240Z","shell.execute_reply.started":"2025-04-30T13:44:07.124339Z","shell.execute_reply":"2025-04-30T13:44:13.123139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/mnist-in-csv/mnist_train.csv\").reset_index(drop=True)\nY = data[\"label\"].to_numpy()\nX = data.drop(\"label\", axis=1).to_numpy()\nX = X.reshape(-1, 1, 28, 28)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:51:06.544991Z","iopub.execute_input":"2025-04-30T15:51:06.545428Z","iopub.status.idle":"2025-04-30T15:51:09.836728Z","shell.execute_reply.started":"2025-04-30T15:51:06.545399Z","shell.execute_reply":"2025-04-30T15:51:09.835605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dimension = 4\nints = np.random.randint(0, X.shape[0], dimension**2, dtype=int)\nfig, axes = plt.subplots(dimension, dimension, figsize=(8, 6))\naxes = axes.ravel()\nfor i in range(dimension**2):\n    axes[i].imshow(X[ints[i], 0, :, :], cmap='gray')  \n    axes[i].set_title(Y[ints[i]])\n    axes[i].axis('off')  \n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:51:12.317003Z","iopub.execute_input":"2025-04-30T15:51:12.317345Z","iopub.status.idle":"2025-04-30T15:51:13.277710Z","shell.execute_reply.started":"2025-04-30T15:51:12.317319Z","shell.execute_reply":"2025-04-30T15:51:13.276744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CNN():\n    def __init__(self, X, Y):\n        new = np.zeros((10, Y.shape[0]))\n        new[Y, range(Y.shape[0])] = 1\n        divide = int(Y.shape[0] * 0.95)\n        self.X = X[:divide]     \n        self.Y = new[:divide]\n        self.X_test = X[divide:]\n        self.Y_test = Y[divide:]\n        self.testing_mode = \"off\"\n        self.filters = []\n        self.alpha, self.beta = [], []\n        self.W = []\n        self.B = []\n        self.activation = []\n        self.old_W, self.old_B, self.old_filter, self.old_alpha, self.old_beta = [], [], [], [], []\n        self.old_WS, self.old_BS, self.old_filterS, self.old_alphaS, self.old_betaS = [], [], [], [], []\n        self.t = 0\n        \n    def create_mini_batch(self, size = 64):\n        if self.testing_mode == \"off\":    \n            indices = np.random.choice(self.X.shape[0], size=size, replace=False)\n            self.X_B = self.X[indices]\n            self.Y_B = self.Y[:, indices]\n        elif self.testing_mode == \"on\":\n            indices = np.random.choice(self.X_test.shape[0], size=size, replace=False)\n            self.X_B = self.X_test[indices]\n            self.Y_B = self.Y_test[:, indices]\n\n    def create_filter_layer(self, filter_dims = (5, 1, 3, 3)):\n        if not self.filters:\n            if self.X_B.shape[1] != filter_dims[1]:\n                raise Exception(\"The filter dimensions do not match\")\n            he_number = np.sqrt(2 / np.prod(filter_dims[2:]))\n            self.filters.append(np.random.randn(*filter_dims))\n            self.old_filter.append(np.zeros((filter_dims)))\n            self.old_filterS.append(np.zeros((filter_dims)))\n        else:\n            if self.filters[-1].shape[0] != filter_dims[1]:\n                raise Exception(\"The filter dimensions do not match\")\n            he_number = np.sqrt(2 / np.prod(filter_dims[2:]))\n            self.filters.append(np.random.randn(*filter_dims))\n            self.old_filter.append(np.zeros((filter_dims)))\n            self.old_filterS.append(np.zeros((filter_dims)))\n            \n    def calculate_input_size(self):\n        if self.filters:\n            starting_dims = self.X_B.shape[2]\n            for i in self.filters:\n                starting_dims -= i.shape[2]\n                starting_dims += 1\n            return starting_dims**2 * i.shape[0]\n        else:\n            raise Exception(\"There are no filters\")\n            \n        \n    def padding(self,Z, padding =2):\n        return np.pad(Z, pad_width=((0,0), (0,0), (padding,padding), (padding,padding)))\n        \n    def fully_connected_layer(self, layer_depth=10, activation=\"ReLU\"):\n        if not self.W:\n            he_number = np.sqrt(2/ self.calculate_input_size())\n            self.W.append(np.random.randn(layer_depth, self.calculate_input_size()) * 10 )\n            self.B.append(np.random.randn(layer_depth, 1))\n            self.alpha.append(np.ones((layer_depth, 1)))\n            self.beta.append(np.zeros((layer_depth, 1)))\n            self.activation.append(activation)\n            self.old_W.append(np.zeros((layer_depth, self.calculate_input_size())))\n            self.old_B.append(np.zeros((layer_depth, 1)))\n            self.old_alpha.append(np.zeros((layer_depth, 1)))\n            self.old_beta.append(np.zeros((layer_depth, 1)))\n            self.old_WS.append(np.zeros((layer_depth, self.calculate_input_size())))\n            self.old_BS.append(np.zeros((layer_depth, 1)))\n            self.old_alphaS.append(np.zeros((layer_depth, 1)))\n            self.old_betaS.append(np.zeros((layer_depth, 1)))\n        else:\n            he_number = np.sqrt(2/ self.W[-1].shape[0])\n            self.W.append(np.random.randn(layer_depth, self.W[-1].shape[0]) * 10)\n            self.B.append(np.random.randn(layer_depth, 1))\n            self.alpha.append(np.ones((layer_depth, 1)))\n            self.beta.append(np.zeros((layer_depth, 1)))\n            self.activation.append(activation)\n            self.old_W.append(np.zeros((layer_depth, self.W[-2].shape[0])))\n            self.old_B.append(np.zeros((layer_depth, 1)))\n            self.old_alpha.append(np.zeros((layer_depth, 1)))\n            self.old_beta.append(np.zeros((layer_depth, 1)))\n            self.old_WS.append(np.zeros((layer_depth, self.W[-2].shape[0])))\n            self.old_BS.append(np.zeros((layer_depth, 1)))\n            self.old_alphaS.append(np.zeros((layer_depth, 1)))\n            self.old_betaS.append(np.zeros((layer_depth, 1)))\n    def convolution(self,Z, f):\n        N, C, H, W = Z.shape\n        NS, CS, HS, WS = Z.strides\n        C_out, _, f_K, f_K = f.shape\n        C_outs, _, _, FWS = f.strides\n        f = np.ascontiguousarray(as_strided(f, shape=(C*f_K**2, C_out), strides=(FWS, C_outs)))\n        inner_dims = f_K * f_K * C\n        A = as_strided(Z, shape=(N, H-f_K+1, W-f_K+1, C, f_K, f_K)\n                      , strides = (NS, HS, WS, CS, HS, WS)).reshape(-1, inner_dims)\n        out = A @ f\n        S, E = out.strides\n        out = as_strided(out, shape=(N, C_out, (H-f_K+1)*(W-f_K+1)), strides=(E*(W-f_K+1)*(H-f_K+1)*C_out, E, S )).reshape(N, C_out, H-f_K+1, W-f_K+1)\n        return out\n    def convolution_backward(self,Z, dZ, f):\n        N, _, H, H = dZ.shape\n        dNs, dCs, dHs, dHs = dZ.strides\n        N, C_in , _, _= Z.shape\n        NS, CS, HS, WS = Z.strides\n        C_out, f_C_in, f_K, f_K = f.shape\n        inner_dims = H * H * N\n        dZ = as_strided(dZ, shape = (C_out, N, H**2), strides=(dCs, dNs ,dHs)).reshape(C_out, inner_dims)\n        Z = as_strided(Z, shape=(C_in, f_K, f_K,N, H, H), strides=(CS, HS, WS, NS, HS, WS)).reshape(-1,inner_dims).T\n        out =  dZ @ Z \n        return out.reshape(f.shape)\n    def convolution_backward_Z(self, dZ, f):\n        dZ = self.padding(dZ)\n        N, C, H, W = dZ.shape\n        NS, CS, HS, WS = dZ.strides\n        f = np.rot90(f, k=2, axes=(2, 3))\n        _, f_C_in, f_K, f_K = f.shape\n        inner_dims = f_K * f_K\n        f = f.reshape(C, f_C_in, f_K**2)\n        dZ = as_strided(dZ, shape=(C, N, H-2, W-2,f_K, f_K), strides=(CS, NS, HS, WS, HS, WS)).reshape(C, -1 , inner_dims)\n        out = np.zeros((N, f_C_in, H-2, W-2))\n        for c in range(C):\n            nout = dZ[c] @ f[c].T\n            out_s, out_e = nout.strides\n            out += np.ascontiguousarray(as_strided(nout, shape=(N, f_C_in, H-2, W-2), strides=(out_e * f_C_in*(H-2)*(W-2), out_e, out_s * (W-2), out_s)))\n        return  out\n    def batch_norm(self, Z):\n        mean = np.mean(Z, axis=1, keepdims=True)\n        var = np.var(Z, axis=1, keepdims=True)\n        out = (Z - mean) / np.sqrt(var+1e-8)\n        return out, mean, var\n    def batch_norm_backward(self,Z, PB, alpha, mean, var, dZ):\n        dalpha = np.sum(dZ * PB, axis=1, keepdims=True)\n        dbeta = np.sum(dZ, axis=1, keepdims = True)\n        dPB = dZ * alpha\n        dvar = np.sum(dPB * (Z - mean) * ( (-1/2) * (var+1e-8)**(-3/2) ), axis=1, keepdims=True)\n        dmean = np.sum(dPB * (-1/np.sqrt(var + 1e-8)), axis=1, keepdims=True) + dvar * (np.sum(-2*(Z - mean), axis=1, keepdims=True)/Z.shape[1])\n        dZ = dPB * (1/np.sqrt(var + 1e-8)) + dvar * ((2*(Z - mean))/Z.shape[1]) + dmean / Z.shape[1]\n        return dZ, dalpha, dbeta\n    def activations(self,Z, activation):\n        if activation == \"ReLU\":\n            return Z  * (Z > 0)\n        if activation == \"softmax\":\n            return np.exp(Z - np.max(Z, axis=0, keepdims=True) + 1e-8) / np.sum(np.exp(Z - np.max(Z, axis=0, keepdims=True)+ 1e-8), axis = 0, keepdims = True)\n    def calculate_loss(self,Z):\n        self.loss = -np.mean(np.sum(np.log(Z + 1e-8) * self.Y_B, axis=0))\n    def forward(self):\n        self.Z_filter = [0 for i in range(len(self.filters))]\n        self.A_filter = [0 for i in range(len(self.filters))]\n        self.Z = [0 for i in range(len(self.W))]\n        self.PB, self.PA = [0 for i in range(len(self.W))], [0 for i in range(len(self.W))]\n        self.A = [0 for i in range(len(self.W))]\n        self.mean, self.var = [0 for i in range(len(self.W))], [0 for i in range(len(self.W))]\n        \n        self.Z_filter[0] = self.convolution(self.X_B, self.filters[0])\n        self.A_filter[0] = self.Z_filter[0] * (self.Z_filter[0] > 0)\n        for i in range(1, len(self.filters)):\n            self.Z_filter[i] = self.convolution(self.A_filter[i-1], self.filters[i])\n            self.A_filter[i] = self.Z_filter[i] * (self.Z_filter[i] > 0)\n        self.Z[0] = self.W[0] @ self.A_filter[-1].reshape(-1, self.calculate_input_size()).T + self.B[0]\n        self.PB[0], self.mean[0], self.var[0] = self.batch_norm(self.Z[0])\n        self.PA[0] = self.PB[0] * self.alpha[0] + self.beta[0]\n        self.A[0] = self.activations(self.PA[0], self.activation[0])\n        for i in range(1,len(self.W)):\n            self.Z[i] = self.W[i] @ self.A[i-1] + self.B[i]\n            self.PB[i], self.mean[i], self.var[i] = self.batch_norm(self.Z[i])\n            self.PA[i] = self.PB[i] * self.alpha[i] + self.beta[i]\n            self.A[i] = self.activations(self.PA[i], self.activation[i])\n        self.calculate_loss(self.A[-1])\n\n    def backward(self):\n        num = len(self.W) - 1\n        self.dW, self.dB, self.dfilter = [0 for i in range(len(self.W))], [0 for i in range(len(self.W))], [0 for i in range(len(self.filters))]\n        self.dalpha, self.dbeta = [0 for i in range(len(self.W))], [0 for i in range(len(self.W))]\n        dZ = (self.A[-1] - self.Y_B) / self.Y_B.shape[1]\n        dZ, self.dalpha[num], self.dbeta[num] = self.batch_norm_backward(self.Z[num], self.PB[num], self.alpha[num], self.mean[num], self.var[num], dZ)\n        self.dW[num] = dZ @ self.A[num-1].T \n        self.dB[num] = np.sum(dZ, axis=1, keepdims=True)\n        dZ = (self.W[num].T @ dZ) * (self.PA[num-1] > 0)\n        for i in range(num-1, 0, -1):\n            dZ, self.dalpha[i], self.dbeta[i] = self.batch_norm_backward(self.Z[i], self.PB[i], self.alpha[i], self.mean[i], self.var[i], dZ) \n            self.dW[i] = dZ @ self.A[i-1].T\n            self.dB[i] = np.sum(dZ, axis=1, keepdims=True)\n            dZ = (self.W[i].T @ dZ) * (self.PA[i-1] > 0)\n        dZ, self.dalpha[0], self.dbeta[0] = self.batch_norm_backward(self.Z[0], self.PB[0], self.alpha[0], self.mean[0], self.var[0], dZ)\n        self.dW[0] = dZ @ self.A_filter[-1].reshape(-1, self.calculate_input_size())\n        dZ = (self.W[0].T @ dZ).T.reshape(self.A_filter[-1].shape) * (self.Z_filter[-1] > 0)\n        for i in range(len(self.filters) -1, 0, -1):\n            self.dfilter[i] = self.convolution_backward(self.A_filter[i-1], dZ, self.filters[i])\n            dZ = self.convolution_backward_Z(dZ, self.filters[i]) * (self.Z_filter[i-1] > 0)\n        self.dfilter[0] = self.convolution_backward(self.X_B, dZ, self.filters[0])\n\n    def adam(self, beta1 = 0.9, beta2 = 0.999):\n        self.t +=1\n        for i in range(len(self.filters)):\n            self.old_filter[i] = (self.old_filter[i] * beta1 + self.dfilter[i] * (1 - beta1)) \n            self.old_filterS[i] = (self.old_filterS[i] * beta1 + self.dfilter[i]**2 * (1 - beta2)) \n            self.dfilter[i] = self.old_filter[i] / np.sqrt(self.old_filterS[i] + 1e-8)\n            \n        for i in range(len(self.W)):\n            self.old_W[i] = (self.old_W[i] * beta1 + self.dW[i] * (1 -beta1)) \n            self.old_B[i] = (self.old_B[i] * beta1 + self.dB[i] * (1 -beta1)) \n            self.old_alpha[i] = (self.old_alpha[i] * beta1 + self.dalpha[i] * (1 -beta1)) \n            self.old_beta[i] = (self.old_beta[i] * beta1 + self.dbeta[i] * (1 -beta1)) \n            self.old_WS[i] = (self.old_WS[i] * beta2 + self.dW[i]**2 * (1 -beta2)) \n            self.old_BS[i] = (self.old_BS[i] * beta2 + self.dB[i]**2 * (1 -beta2)) \n            self.old_alphaS[i] = (self.old_alphaS[i] * beta2 + self.dalpha[i]**2 * (1 -beta2)) \n            self.old_betaS[i] = (self.old_betaS[i] * beta2 + self.dbeta[i]**2 * (1 -beta2)) \n            self.dW[i] = self.old_W[i] / np.sqrt(self.old_WS[i] + 1e-8)\n            self.dB[i] = self.old_B[i] / np.sqrt(self.old_BS[i] + 1e-8)\n            self.dalpha[i] = self.old_alpha[i] / np.sqrt(self.old_alphaS[i] + 1e-8)\n            self.dbeta[i] = self.old_beta[i] / np.sqrt(self.old_betaS[i] + 1e-8)\n            \n            \n    def update(self, learning_rate=0.01):\n        for i in range(len(self.filters)):\n            self.filters[i] = self.filters[i] - learning_rate * self.dfilter[i]\n        for i in range(len(self.W)):\n            self.W[i] = self.W[i] - learning_rate * self.dW[i]\n            self.B[i] = self.B[i] - learning_rate * self.dB[i]\n            self.alpha[i] = self.alpha[i] - learning_rate * self.dalpha[i]\n            self.beta[i] = self.beta[i] - learning_rate * self.dbeta[i]\n            \n        \n            \n        \n        \n        \n        \n            \n            \n            \n                \n                \n            \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T15:58:05.158102Z","iopub.execute_input":"2025-04-30T15:58:05.159310Z","iopub.status.idle":"2025-04-30T15:58:05.221269Z","shell.execute_reply.started":"2025-04-30T15:58:05.159221Z","shell.execute_reply":"2025-04-30T15:58:05.220172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nn = CNN(X, Y)\nnn.create_mini_batch()\nnn.create_filter_layer()\nnn.create_filter_layer(filter_dims = (32, 5, 3, 3))\nnn.create_filter_layer(filter_dims = (64, 32, 3, 3))\nnn.fully_connected_layer(layer_depth=64, activation=\"ReLU\")\nnn.fully_connected_layer(layer_depth=32, activation=\"ReLU\")\nnn.fully_connected_layer(layer_depth=10, activation=\"softmax\")\nnn.forward()\nnn.backward()\nold_loss = nn.loss\nnn.filters[1][7, 2, 2, 0] += 1e-6\nnn.forward()\nprint(\"manually calculated gradient:\",(nn.loss - old_loss)/ 1e-6,\n      \"\\nnetwork calculated gradient\", nn.dfilter[1][7, 2, 2, 0],\n     \"\\ndifference\", np.absolute((nn.loss - old_loss)/ 1e-6 - nn.dfilter[1][7, 2, 2, 0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:37:57.272881Z","iopub.execute_input":"2025-04-30T16:37:57.273223Z","iopub.status.idle":"2025-04-30T16:37:58.802035Z","shell.execute_reply.started":"2025-04-30T16:37:57.273196Z","shell.execute_reply":"2025-04-30T16:37:58.800953Z"}},"outputs":[{"name":"stdout","text":"manually calculated gradient: 0.001033815116846526 \nnetwork calculated gradient 0.001033815008118813 \ndifference 1.0872771303085949e-10\n","output_type":"stream"}],"execution_count":131},{"cell_type":"code","source":"l = 1e-6\nd = 0\nnn.testing_mode = \"off\"\nloss_values = np.zeros((10))\nfor i in range(100):\n    nn.forward()\n    nn.backward()\n    nn.adam()\n    nn.update(learning_rate=lo)\n    nn.create_mini_batch()\n    loss_values[d] = nn.loss\n    d+= 1\n    if d % 10 == 0:\n        print(np.median(loss_values))\n        d = 0\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:23:24.124033Z","iopub.execute_input":"2025-04-30T16:23:24.124517Z","iopub.status.idle":"2025-04-30T16:25:42.987280Z","shell.execute_reply.started":"2025-04-30T16:23:24.124480Z","shell.execute_reply":"2025-04-30T16:25:42.986147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nn.testing_mode = \"on\"\nnn.create_mini_batch()\nnn.forward()\nvalues = np.argmax(nn.A[-1], axis=0)\nactual_values = np.argmax(nn.Y_B, axis=0)\nnum_differences = np.sum(values != actual_values)\nprint(f\"accuracy:{(64 - num_differences)/64}, correct:{(64 - num_differences)}/64\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:27:58.745381Z","iopub.execute_input":"2025-04-30T16:27:58.745716Z","iopub.status.idle":"2025-04-30T16:27:58.849448Z","shell.execute_reply.started":"2025-04-30T16:27:58.745691Z","shell.execute_reply":"2025-04-30T16:27:58.848283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dimension = 8\nfig, axes = plt.subplots(dimension, dimension, figsize=(8, 6))\naxes = axes.ravel()\nfor i in range(dimension**2):\n    axes[i].imshow(nn.X_B[i, 0, :, :], cmap='gray')  \n    axes[i].set_title(f\"p:{values[i]}, l:{actual_values[i]}\")\n    axes[i].axis('off')  \n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:28:01.349515Z","iopub.execute_input":"2025-04-30T16:28:01.349920Z","iopub.status.idle":"2025-04-30T16:28:05.079919Z","shell.execute_reply.started":"2025-04-30T16:28:01.349888Z","shell.execute_reply":"2025-04-30T16:28:05.078674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mismatch = (values == actual_values).astype(int).tolist()\nmismatch = [i for i in range(len(mismatch)) if mismatch[i] == 0]\nfig, axes = plt.subplots(1, num_differences, figsize=(8, 6))\naxes = axes.ravel()\nfor i in range(len(mismatch)):\n    axes[i].imshow(nn.X_B[mismatch[i], 0, :, :], cmap='gray')  \n    axes[i].set_title(f\"p:{values[mismatch[i]]}, l:{actual_values[mismatch[i]]}\")\n    axes[i].axis('off')  \n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:28:07.127860Z","iopub.execute_input":"2025-04-30T16:28:07.128267Z","iopub.status.idle":"2025-04-30T16:28:07.337820Z","shell.execute_reply.started":"2025-04-30T16:28:07.128239Z","shell.execute_reply":"2025-04-30T16:28:07.336720Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}